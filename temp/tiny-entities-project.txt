# Tiny Entities Project Structure

## Project Overview
```
tiny-entities/
├── .github/
│   └── workflows/
│       └── test.yml
├── src/
│   ├── __init__.py
│   ├── world/
│   │   ├── __init__.py
│   │   ├── physics.py
│   │   ├── map_generator.py
│   │   ├── sound_system.py
│   │   └── non_deterministic.py
│   ├── creatures/
│   │   ├── __init__.py
│   │   ├── brain.py
│   │   ├── mood_system.py
│   │   ├── perception.py
│   │   ├── action_selection.py
│   │   └── internal_state.py
│   ├── emergence/
│   │   ├── __init__.py
│   │   ├── music_analyzer.py
│   │   ├── communication_detector.py
│   │   └── mapping_evaluator.py
│   └── simulation/
│       ├── __init__.py
│       ├── main_loop.py
│       ├── entropy_rewards.py
│       ├── data_collector.py
│       └── visualization.py
├── config/
│   ├── __init__.py
│   ├── api_config.py
│   └── simulation_config.py
├── examples/
│   ├── basic_simulation.py
│   └── analysis_example.py
├── tests/
│   ├── __init__.py
│   ├── test_world.py
│   ├── test_creatures.py
│   └── test_emergence.py
├── .env.example
├── .gitignore
├── README.md
├── requirements.txt
├── setup.py
└── claude_code_config.json
```

## File Contents

### README.md
```markdown
# Tiny Entities - Emergent Artificial Life Simulation

Simple creatures with emergent music, communication, and mapping behaviors.

## Overview

This project simulates artificial life forms that develop emergent behaviors through entropy-driven learning. Creatures have simple brains with mood systems that emerge from reward prediction errors, leading to collective behaviors like music-making, communication, and spatial mapping.

## Key Features

- **Emergent Mood System**: Creatures develop emotional states based on reward prediction errors
- **Entropy-Driven Learning**: Surprise and novelty drive exploration and token rewards
- **Collective Music**: Creatures may spontaneously coordinate sounds into music
- **Communication Emergence**: Simple signals can develop into proto-communication
- **Spatial Mapping**: Creatures learn to navigate and understand their environment

## Installation

```bash
git clone https://github.com/yourusername/tiny-entities.git
cd tiny-entities
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your API keys
```

## Quick Start

```python
from src.simulation.main_loop import EmergentLifeSimulation

# Create simulation with 8 creatures
sim = EmergentLifeSimulation(num_creatures=8)

# Run simulation
import asyncio
asyncio.run(sim.run_simulation())
```

## Configuration

Edit `config/simulation_config.py` to adjust:
- World size and physics
- Creature parameters
- Emergence detection thresholds
- API model choices

## API Keys Required

- `OPENROUTER_API_KEY` - For LLM access (action selection)
- `ANTHROPIC_API_KEY` - For Claude analysis (optional)
- `HUGGINGFACE_API_KEY` - For free model access

## Architecture

### Creature Brain
- Minimal cognitive architecture
- Mood emerges from reward prediction errors
- Action selection via simple LLM or rules
- Limited vocabulary for internal monologue

### World Model
- 2D grid with Conway-like dynamics
- Sound propagation system
- Non-deterministic action acceptance
- Food and obstacle generation

### Emergence Detection
- External AI analyzes collective behaviors
- Music quality assessment
- Communication pattern detection
- Spatial understanding evaluation

## Development

```bash
# Run tests
pytest

# Run with visualization
python examples/basic_simulation.py --visualize

# Analyze emergence
python examples/analysis_example.py
```

## License

MIT
```

### requirements.txt
```
numpy>=1.24.0
asyncio
aiohttp>=3.8.0
python-dotenv>=1.0.0
matplotlib>=3.6.0
pygame>=2.5.0
openai>=1.0.0
anthropic>=0.3.0
huggingface-hub>=0.19.0
pytest>=7.4.0
pytest-asyncio>=0.21.0
rich>=13.0.0
pandas>=2.0.0
scipy>=1.10.0
```

### .env.example
```bash
# API Keys
OPENROUTER_API_KEY=your_openrouter_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
HUGGINGFACE_API_KEY=your_huggingface_key_here
OPENAI_API_KEY=your_openai_key_here
GROQ_API_KEY=your_groq_key_here

# Simulation Settings
MAX_DAILY_COST_EUR=2.0
DEFAULT_FREE_MODEL=huggingface/meta-llama/Llama-2-7b-chat-hf
DEFAULT_ANALYSIS_MODEL=anthropic/claude-3-sonnet-20240229

# Debug
DEBUG_MODE=false
LOG_LEVEL=INFO
```

### claude_code_config.json
```json
{
  "name": "tiny-entities",
  "description": "Emergent artificial life simulation with mood-based cognition",
  "version": "0.1.0",
  "entry_point": "src/simulation/main_loop.py",
  "python_version": "3.9",
  "environment_variables": [
    "OPENROUTER_API_KEY",
    "ANTHROPIC_API_KEY",
    "HUGGINGFACE_API_KEY"
  ],
  "dependencies": {
    "file": "requirements.txt"
  },
  "commands": {
    "run": "python -m src.simulation.main_loop",
    "test": "pytest",
    "visualize": "python examples/basic_simulation.py --visualize",
    "analyze": "python examples/analysis_example.py"
  },
  "claude_code": {
    "auto_install_deps": true,
    "watch_files": ["src/**/*.py"],
    "restart_on_change": true
  }
}
```

### setup.py
```python
from setuptools import setup, find_packages

setup(
    name="tiny-entities",
    version="0.1.0",
    author="Your Name",
    description="Emergent artificial life simulation",
    packages=find_packages(),
    install_requires=[
        "numpy>=1.24.0",
        "aiohttp>=3.8.0",
        "python-dotenv>=1.0.0",
        "matplotlib>=3.6.0",
    ],
    python_requires=">=3.8",
)
```

### .gitignore
```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
.venv

# Environment
.env
.env.local

# IDE
.vscode/
.idea/
*.swp
*.swo

# Project specific
data/
logs/
outputs/
*.log
simulation_saves/
emergence_reports/

# OS
.DS_Store
Thumbs.db
```

### src/__init__.py
```python
"""Tiny Entities - Emergent Artificial Life Simulation"""

__version__ = "0.1.0"
```

### src/world/__init__.py
```python
from .physics import SimpleWorld
from .non_deterministic import NonDeterministicWorldModel
from .sound_system import SoundLayer

__all__ = ['SimpleWorld', 'NonDeterministicWorldModel', 'SoundLayer']
```

### src/world/physics.py
```python
import numpy as np
from typing import Dict, List, Tuple, Optional

class SimpleWorld:
    """Conway's Game of Life inspired 2D world with resources"""
    
    def __init__(self, width: int = 100, height: int = 100):
        self.width = width
        self.height = height
        
        # World state: 0=empty, 1=food, 2=obstacle, 3=creature
        self.grid = np.zeros((height, width), dtype=int)
        
        # Sound layer - simple frequency/amplitude at each location
        self.sound_grid = np.zeros((height, width, 2))  # [frequency, amplitude]
        
        # Spawn some food randomly
        self._spawn_food(density=0.1)
        self._spawn_obstacles(density=0.05)
    
    def _spawn_food(self, density: float):
        """Randomly place food in world"""
        food_count = int(self.width * self.height * density)
        for _ in range(food_count):
            x, y = np.random.randint(0, self.width), np.random.randint(0, self.height)
            if self.grid[y, x] == 0:  # Empty space
                self.grid[y, x] = 1
    
    def _spawn_obstacles(self, density: float):
        """Place obstacles that block movement"""
        obstacle_count = int(self.width * self.height * density)
        for _ in range(obstacle_count):
            x, y = np.random.randint(0, self.width), np.random.randint(0, self.height)
            if self.grid[y, x] == 0:
                self.grid[y, x] = 2
    
    def get_local_view(self, x: int, y: int, radius: int = 5) -> Dict:
        """Get creature's local perception"""
        # Visual perception - grid around creature
        x1, x2 = max(0, x-radius), min(self.width, x+radius+1)
        y1, y2 = max(0, y-radius), min(self.height, y+radius+1)
        
        visual = self.grid[y1:y2, x1:x2].copy()
        
        # Audio perception - sounds in area
        sound = self.sound_grid[y1:y2, x1:x2].copy()
        
        return {
            'visual': visual,
            'sound': sound,
            'position': (x, y),
            'local_bounds': (x1, y1, x2, y2)
        }
    
    def update_sound(self, x: int, y: int, frequency: float, amplitude: float):
        """Creature makes sound at location"""
        if 0 <= x < self.width and 0 <= y < self.height:
            # Sound decays over distance
            for dy in range(-3, 4):
                for dx in range(-3, 4):
                    nx, ny = x + dx, y + dy
                    if 0 <= nx < self.width and 0 <= ny < self.height:
                        distance = np.sqrt(dx*dx + dy*dy)
                        decay = max(0, 1 - distance/3)
                        self.sound_grid[ny, nx, 0] += frequency * decay
                        self.sound_grid[ny, nx, 1] += amplitude * decay
    
    def step(self):
        """Update world physics each timestep"""
        # Sound decay
        self.sound_grid *= 0.9
        
        # Occasionally spawn new food
        if np.random.random() < 0.01:
            self._spawn_food(0.005)
```

### src/creatures/mood_system.py
```python
import numpy as np
from typing import Dict, List

class EmergentMoodSystem:
    """Mood emerges from reward prediction errors and accumulated experience"""
    
    def __init__(self):
        # Core state
        self.valence = 0.0  # -1 to 1, emerges from reward history
        self.arousal = 0.5  # 0 to 1, intensity/activation level
        
        # Reward prediction system
        self.expected_reward = 0.0
        self.reward_history = []  # Recent rewards
        self.reward_baseline = 0.0  # Long-term average
        
        # Association memory - what situations led to what outcomes
        self.situation_outcomes = {}  # Hash of situation -> list of outcomes
        
        # Learning rates
        self.fast_learning = 0.1  # For arousal (immediate response)
        self.slow_learning = 0.01  # For valence (stable mood)
        
    def process_experience(self, situation: Dict, actual_reward: float):
        """Update mood based on prediction error"""
        
        # Calculate prediction error (surprise)
        prediction_error = actual_reward - self.expected_reward
        
        # Update arousal based on absolute prediction error
        # Big surprises (good or bad) increase arousal
        arousal_change = abs(prediction_error) * self.fast_learning
        self.arousal = min(1.0, self.arousal + arousal_change)
        
        # Update valence based on signed prediction error
        # Better than expected → positive mood
        # Worse than expected → negative mood
        valence_change = prediction_error * self.slow_learning
        self.valence = np.clip(self.valence + valence_change, -1.0, 1.0)
        
        # Update reward prediction for next time
        situation_key = self._hash_situation(situation)
        if situation_key not in self.situation_outcomes:
            self.situation_outcomes[situation_key] = []
        
        self.situation_outcomes[situation_key].append(actual_reward)
        
        # Update expected reward based on similar past situations
        self.expected_reward = self._predict_reward(situation)
        
        # Arousal naturally decays over time
        self.arousal *= 0.99
        
        return {
            'prediction_error': prediction_error,
            'new_valence': self.valence,
            'new_arousal': self.arousal
        }
    
    def _hash_situation(self, situation: Dict) -> str:
        """Create a simple hash of the situation for memory"""
        # Simple categorical representation
        key_parts = []
        
        # Visual features
        if 'food_nearby' in situation:
            key_parts.append(f"food_{situation['food_nearby']}")
        if 'creatures_nearby' in situation:
            key_parts.append(f"creatures_{situation['creatures_nearby']}")
        if 'sound_level' in situation:
            # Discretize continuous values
            sound_category = 'quiet' if situation['sound_level'] < 0.3 else 'loud'
            key_parts.append(f"sound_{sound_category}")
        
        return "_".join(key_parts)
    
    def _predict_reward(self, situation: Dict) -> float:
        """Predict reward based on past experiences"""
        situation_key = self._hash_situation(situation)
        
        if situation_key in self.situation_outcomes:
            # Use recent average for this situation
            recent_outcomes = self.situation_outcomes[situation_key][-10:]
            return np.mean(recent_outcomes)
        else:
            # Unknown situation - use general baseline
            return self.reward_baseline
```

### src/creatures/brain.py
```python
import numpy as np
from typing import Dict, List
from .mood_system import EmergentMoodSystem

class EnhancedBrain:
    """Brain with emergent mood based on reward learning"""
    
    def __init__(self, creature_id: str):
        self.creature_id = creature_id
        
        # Basic survival
        self.health = 100.0
        self.energy = 100.0
        
        # Emergent mood system
        self.mood_system = EmergentMoodSystem()
        
        # Action tokens from surprise
        self.action_tokens = 10
        self.max_tokens = 50
        
        # Memory for surprise calculation
        self.perception_memory = []
        self.max_memory = 20
        
        # Learned action preferences based on outcomes
        self.action_values = {}  # action -> expected value
        
        # Basic vocabulary for internal monologue
        self.vocabulary = [
            'food', 'obstacle', 'creature', 'sound', 'move', 'stay',
            'hungry', 'safe', 'danger', 'curious', 'tired', 'excited',
            'good', 'bad', 'expect', 'need', 'want', 'fear'
        ]
        
    def process_timestep(self, perception: Dict, action_taken: str, 
                        outcome: Dict) -> Dict:
        """Complete cognitive cycle with emergent mood"""
        
        # 1. Calculate surprise from perception
        surprise = self._calculate_perceptual_surprise(perception)
        
        # 2. Calculate total reward (surprise + survival outcomes)
        reward = self._calculate_total_reward(surprise, outcome)
        
        # 3. Create situation representation
        situation = {
            'food_nearby': perception.get('food_count', 0) > 0,
            'creatures_nearby': perception.get('creature_count', 0),
            'sound_level': perception.get('sound_level', 0.0),
            'health': self.health,
            'energy': self.energy,
            'last_action': action_taken
        }
        
        # 4. Update mood based on reward prediction error
        mood_update = self.mood_system.process_experience(situation, reward)
        
        # 5. Update action values based on outcome
        self._update_action_values(action_taken, reward)
        
        # 6. Gain tokens from surprise
        tokens_gained = int(surprise * 10)
        self.action_tokens = min(self.max_tokens, 
                               self.action_tokens + tokens_gained)
        
        # 7. Update health/energy based on outcome
        if outcome.get('effect') == 'found_food':
            self.health = min(100, self.health + 20)
            self.energy = min(100, self.energy + 15)
        
        # Energy depletes with action
        self.energy = max(0, self.energy - 1)
        
        return {
            'surprise': surprise,
            'reward': reward,
            'mood_valence': self.mood_system.valence,
            'mood_arousal': self.mood_system.arousal,
            'tokens_gained': tokens_gained,
            'prediction_error': mood_update['prediction_error']
        }
    
    def _calculate_perceptual_surprise(self, perception: Dict) -> float:
        """Calculate surprise from perception changes"""
        if len(self.perception_memory) < 3:
            return 0.8  # High surprise for novel experiences
        
        # Compare to recent perceptions
        recent = self.perception_memory[-3:]
        
        # Calculate difference in perceptual features
        total_diff = 0.0
        
        for past in recent:
            # Visual differences
            diff = abs(perception.get('food_count', 0) - 
                      past.get('food_count', 0))
            diff += abs(perception.get('creature_count', 0) - 
                       past.get('creature_count', 0))
            
            # Audio differences
            diff += abs(perception.get('sound_level', 0) - 
                       past.get('sound_level', 0)) * 2  # Weight sound changes
            
            total_diff += diff
        
        # Normalize
        surprise = min(1.0, total_diff / (len(recent) * 5))
        
        # Add to memory
        self.perception_memory.append(perception)
        if len(self.perception_memory) > self.max_memory:
            self.perception_memory.pop(0)
        
        return surprise
    
    def _calculate_total_reward(self, surprise: float, outcome: Dict) -> float:
        """Calculate total reward from surprise and survival outcomes"""
        reward = 0.0
        
        # Surprise is inherently rewarding (curiosity drive)
        reward += surprise * 0.5
        
        # Survival rewards
        if outcome.get('effect') == 'found_food':
            # Food more valuable when hungry
            food_value = 1.0 if self.health < 50 else 0.3
            reward += food_value
        
        # Social interaction bonus (if near other creatures)
        if outcome.get('near_creatures', 0) > 0:
            # Baseline positive for social contact
            reward += 0.2
            
            # But negative if harmed (would need to track this)
            if outcome.get('harmed_by_creature'):
                reward -= 1.0
        
        # Making sounds gets small reward if others respond
        if 'made_sound' in str(outcome.get('effect', '')):
            if outcome.get('sound_responded_to'):
                reward += 0.3
        
        return reward
    
    def _update_action_values(self, action: str, reward: float):
        """Update expected value of actions based on outcomes"""
        if action not in self.action_values:
            self.action_values[action] = 0.0
        
        # Simple Q-learning update
        learning_rate = 0.1
        self.action_values[action] += learning_rate * (reward - self.action_values[action])
    
    def get_action_bias(self) -> Dict[str, float]:
        """Get action preferences based on mood and learned values"""
        biases = {}
        
        # Mood affects exploration vs exploitation
        exploration_tendency = self.mood_system.arousal  # High arousal = explore
        
        # Positive valence = approach behaviors
        # Negative valence = avoidance behaviors
        if self.mood_system.valence > 0.2:
            # Positive mood - more interactive
            biases['make_sound_high'] = 0.3
            biases['move_toward_creatures'] = 0.2
            biases['explore'] = 0.3 * exploration_tendency
        elif self.mood_system.valence < -0.2:
            # Negative mood - more cautious
            biases['stay'] = 0.3
            biases['move_away'] = 0.2
            biases['make_sound_low'] = 0.1  # Distress call
        
        # Blend with learned action values
        for action, value in self.action_values.items():
            if action in biases:
                biases[action] = 0.7 * biases[action] + 0.3 * value
            else:
                biases[action] = value
        
        return biases
    
    def generate_internal_monologue(self) -> str:
        """Create internal narrative based on mood and state"""
        words = []
        
        # Mood colors perception
        if self.mood_system.valence > 0.3:
            if self.mood_system.arousal > 0.6:
                words.append("excited good")
            else:
                words.append("content safe")
        elif self.mood_system.valence < -0.3:
            if self.mood_system.arousal > 0.6:
                words.append("danger worried")
            else:
                words.append("tired sad")
        
        # Survival needs
        if self.health < 30:
            words.append("very hungry need food")
        elif self.health < 60:
            words.append("hungry")
        
        if self.energy < 20:
            words.append("exhausted must rest")
        
        # Recent experiences affect narrative
        if self.mood_system.expected_reward > 0.5:
            words.append("expect good")
        elif self.mood_system.expected_reward < -0.3:
            words.append("expect bad")
        
        return " ".join(words) if words else "..."
```

### config/api_config.py
```python
import os
from dotenv import load_dotenv

load_dotenv()

class APIConfig:
    """API configuration and model selection"""
    
    # API Keys
    OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')
    ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
    HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')
    
    # Model choices
    FREE_ACTION_MODEL = os.getenv('DEFAULT_FREE_MODEL', 'huggingface/meta-llama/Llama-2-7b-chat-hf')
    ANALYSIS_MODEL = os.getenv('DEFAULT_ANALYSIS_MODEL', 'anthropic/claude-3-sonnet-20240229')
    
    # Cost limits
    MAX_DAILY_COST_EUR = float(os.getenv('MAX_DAILY_COST_EUR', '2.0'))
    
    @classmethod
    def get_action_model(cls):
        """Get model for creature action selection"""
        if cls.HUGGINGFACE_API_KEY:
            return cls.FREE_ACTION_MODEL
        elif cls.OPENROUTER_API_KEY:
            return "openrouter/auto"  # Let OpenRouter choose cheapest
        else:
            return None  # Fall back to rule-based
    
    @classmethod
    def get_analysis_model(cls):
        """Get model for emergence analysis"""
        if cls.ANTHROPIC_API_KEY:
            return cls.ANALYSIS_MODEL
        elif cls.OPENROUTER_API_KEY:
            return "openrouter/anthropic/claude-3-sonnet"
        else:
            return None
```

### examples/basic_simulation.py
```python
#!/usr/bin/env python3
"""Basic simulation example with optional visualization"""

import asyncio
import argparse
from src.simulation.main_loop import EmergentLifeSimulation
from src.simulation.visualization import SimulationVisualizer

async def main():
    parser = argparse.ArgumentParser(description='Run tiny entities simulation')
    parser.add_argument('--creatures', type=int, default=8, help='Number of creatures')
    parser.add_argument('--steps', type=int, default=10000, help='Simulation steps')
    parser.add_argument('--visualize', action='store_true', help='Enable visualization')
    parser.add_argument('--analyze-every', type=int, default=500, help='Steps between analysis')
    
    args = parser.parse_args()
    
    # Create simulation
    sim = EmergentLifeSimulation(
        num_creatures=args.creatures,
        max_steps=args.steps,
        analyze_every=args.analyze_every
    )
    
    if args.visualize:
        # Run with visualization
        visualizer = SimulationVisualizer(sim)
        await visualizer.run()
    else:
        # Run headless
        await sim.run_simulation()

if __name__ == "__main__":
    asyncio.run(main())
```

### src/simulation/main_loop.py
```python
import asyncio
import numpy as np
from typing import List, Dict
from ..world.non_deterministic import NonDeterministicWorldModel
from ..creatures.brain import EnhancedBrain
from ..creatures.action_selection import MoodInfluencedActionSelector
from ..emergence.music_analyzer import MusicEmergenceAnalyzer
from ..config.api_config import APIConfig

class EmergentLifeSimulation:
    """Main simulation loop for artificial life"""
    
    def __init__(self, num_creatures: int = 5, max_steps: int = 10000, 
                 analyze_every: int = 500):
        self.world_model = NonDeterministicWorldModel()
        self.creatures = self._create_creatures(num_creatures)
        self.music_analyzer = MusicEmergenceAnalyzer()
        self.sound_history = []
        
        self.step_count = 0
        self.max_steps = max_steps
        self.analyze_every = analyze_every
        
        # Cost tracking
        self.daily_cost_eur = 0.0
        self.api_config = APIConfig()
    
    def _create_creatures(self, num: int) -> List[Dict]:
        """Create simple creatures in world"""
        creatures = []
        for i in range(num):
            creature = {
                'id': f'creature_{i}',
                'brain': EnhancedBrain(f'creature_{i}'),
                'action_selector': MoodInfluencedActionSelector(),
                'position': (
                    np.random.randint(10, self.world_model.world.width - 10),
                    np.random.randint(10, self.world_model.world.height - 10)
                ),
                'alive': True
            }
            creatures.append(creature)
        return creatures
    
    async def simulation_step(self):
        """Single simulation step"""
        
        for creature in self.creatures:
            if not creature['alive']:
                continue
                
            # Skip if no action tokens
            if creature['brain'].action_tokens <= 0:
                continue
            
            # Get perception
            world_view = self.world_model.world.get_local_view(*creature['position'])
            
            # Process perception
            perception = self._process_perception(world_view)
            
            # Generate internal monologue
            thoughts = creature['brain'].generate_internal_monologue()
            
            # Select action based on mood and situation
            action = creature['action_selector'].select_action(
                creature['brain'], perception
            )
            
            # Execute action through world model
            action_result = self.world_model.propose_action(
                creature['id'], action, creature['position']
            )
            
            # Process complete timestep
            cognitive_result = creature['brain'].process_timestep(
                perception, action, action_result
            )
            
            # Update creature state
            if action_result['accepted']:
                creature['position'] = action_result['new_position']
                creature['brain'].action_tokens -= 1
                
                # Log sounds for music analysis
                if 'sound' in action:
                    self.sound_history.append({
                        'creature_id': creature['id'],
                        'frequency': 0.3 if 'low' in action else 0.7,
                        'x': creature['position'][0],
                        'y': creature['position'][1],
                        'step': self.step_count,
                        'mood_valence': creature['brain'].mood_system.valence,
                        'mood_arousal': creature['brain'].mood_system.arousal
                    })
            
            # Check if creature dies
            if creature['brain'].health <= 0:
                creature['alive'] = False
        
        # World physics step
        self.world_model.world.step()
        self.step_count += 1
    
    def _process_perception(self, world_view: Dict) -> Dict:
        """Convert world view to creature perception"""
        visual = world_view['visual']
        sound = world_view['sound']
        
        return {
            'food_count': np.sum(visual == 1),
            'obstacle_count': np.sum(visual == 2),
            'creature_count': np.sum(visual == 3),
            'sound_level': np.mean(sound[:,:,1]),  # Average amplitude
            'sound_freq': np.mean(sound[:,:,0])    # Average frequency
        }
    
    async def analyze_emergence(self):
        """Periodically analyze emergent behaviors"""
        if len(self.sound_history) < 10:
            return
            
        print(f"\n--- Emergence Analysis at Step {self.step_count} ---")
        
        # Analyze music emergence
        if self.api_config.get_analysis_model():
            music_analysis = await self.music_analyzer.analyze_collective_music(
                self.sound_history[-100:]  # Last 100 sounds
            )
            
            print(f"Music Score: {music_analysis.get('music_score', 'N/A')}/10")
            print(f"Coordination: {music_analysis.get('coordination_detected', 'N/A')}")
            
            # Update cost tracking
            analysis_cost = 0.01  # Estimate
            self.daily_cost_eur += analysis_cost
            print(f"Daily cost so far: €{self.daily_cost_eur:.3f}")
        else:
            print("No analysis model configured - skipping emergence analysis")
        
        # Print creature mood summary
        print("\nCreature Moods:")
        for creature in self.creatures:
            if creature['alive']:
                mood = creature['brain'].mood_system
                print(f"  {creature['id']}: valence={mood.valence:.2f}, "
                      f"arousal={mood.arousal:.2f}, tokens={creature['brain'].action_tokens}")
    
    async def run_simulation(self):
        """Run the complete simulation"""
        print("Starting Emergent Life Simulation...")
        print(f"World size: {self.world_model.world.width}x{self.world_model.world.height}")
        print(f"Creatures: {len(self.creatures)}")
        print(f"Max steps: {self.max_steps}")
        
        while self.step_count < self.max_steps:
            await self.simulation_step()
            
            # Periodic analysis
            if self.step_count % self.analyze_every == 0:
                await self.analyze_emergence()
            
            # Simple progress
            if self.step_count % 1000 == 0:
                alive_count = sum(1 for c in self.creatures if c['alive'])
                print(f"Step {self.step_count}/{self.max_steps} - {alive_count} creatures alive")
            
            # Check daily cost limit
            if self.daily_cost_eur >= self.api_config.MAX_DAILY_COST_EUR:
                print(f"Daily cost limit reached (€{self.daily_cost_eur:.2f})")
                break
        
        print("\nSimulation complete!")
        print(f"Final step: {self.step_count}")
        print(f"Total cost: €{self.daily_cost_eur:.3f}")
```
